diff --git a/src/transformers/models/gpt2/modeling_gpt2.py b/src/transformers/models/gpt2/modeling_gpt2.py
index d6fab7f7f..a70faf505 100644
--- a/src/transformers/models/gpt2/modeling_gpt2.py
+++ b/src/transformers/models/gpt2/modeling_gpt2.py
@@ -581,8 +581,11 @@ class GPT2Model(GPT2PreTrainedModel):
         super().__init__(config)
 
         self.embed_dim = config.hidden_size
+        self.num_heads = config.num_attention_heads
+        self.num_layers = config.n_layer
+        self.head_dim = self.embed_dim // self.num_heads
 
-        self.wte = nn.Embedding(config.vocab_size, self.embed_dim)
+        # self.wte = nn.Embedding(config.vocab_size, self.embed_dim)
         self.wpe = nn.Embedding(config.max_position_embeddings, self.embed_dim)
 
         self.drop = nn.Dropout(config.embd_pdrop)
@@ -606,7 +609,7 @@ class GPT2Model(GPT2PreTrainedModel):
         self.model_parallel = True
         self.first_device = "cpu" if "cpu" in self.device_map.keys() else "cuda:" + str(min(self.device_map.keys()))
         self.last_device = "cuda:" + str(max(self.device_map.keys()))
-        self.wte = self.wte.to(self.first_device)
+        # self.wte = self.wte.to(self.first_device)
         self.wpe = self.wpe.to(self.first_device)
         # Load onto devices
         for k, v in self.device_map.items():
@@ -622,15 +625,15 @@ class GPT2Model(GPT2PreTrainedModel):
         self.device_map = None
         self.first_device = "cpu"
         self.last_device = "cpu"
-        self.wte = self.wte.to("cpu")
+        # self.wte = self.wte.to("cpu")
         self.wpe = self.wpe.to("cpu")
         for index in range(len(self.h)):
             self.h[index] = self.h[index].to("cpu")
         self.ln_f = self.ln_f.to("cpu")
         torch.cuda.empty_cache()
 
-    def get_input_embeddings(self):
-        return self.wte
+    # def get_input_embeddings(self):
+    #    return self.wte
 
     def set_input_embeddings(self, new_embeddings):
         self.wte = new_embeddings
@@ -652,7 +655,8 @@ class GPT2Model(GPT2PreTrainedModel):
     def forward(
         self,
         input_ids=None,
-        past_key_values=None,
+        past_values=None,
+        past_keys=None,
         attention_mask=None,
         token_type_ids=None,
         position_ids=None,
@@ -691,11 +695,18 @@ class GPT2Model(GPT2PreTrainedModel):
         if position_ids is not None:
             position_ids = position_ids.view(-1, input_shape[-1])
 
-        if past_key_values is None:
+        if past_keys is None and past_values is None:
             past_length = 0
-            past_key_values = tuple([None] * len(self.h))
+        elif past_keys is None and past_values is not None:
+            raise ValueError("You have to specify both past_keys and past_values")
+        elif past_keys is not None and past_values is None:
+            raise ValueError("You have to specify both past_keys and past_values")
         else:
-            past_length = past_key_values[0][0].size(-2)
+            assert (
+                    past_keys.shape == past_values.shape
+            ), f"Past keys shape {past_keys.shape} and array shape {past_values.shape} mismatched"
+            past_length = past_keys.size()[1]
+
         if position_ids is None:
             position_ids = torch.arange(past_length, input_shape[-1] + past_length, dtype=torch.long, device=device)
             position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])
@@ -737,24 +748,31 @@ class GPT2Model(GPT2PreTrainedModel):
         # head_mask has shape n_layer x batch x n_heads x N x N
         head_mask = self.get_head_mask(head_mask, self.config.n_layer)
 
-        if inputs_embeds is None:
-            inputs_embeds = self.wte(input_ids)
+        # if inputs_embeds is None:
+        #    inputs_embeds = self.wte(input_ids)
         position_embeds = self.wpe(position_ids)
         hidden_states = inputs_embeds + position_embeds
 
-        if token_type_ids is not None:
-            token_type_embeds = self.wte(token_type_ids)
-            hidden_states = hidden_states + token_type_embeds
+        # if token_type_ids is not None:
+        #    token_type_embeds = self.wte(token_type_ids)
+        #    hidden_states = hidden_states + token_type_embeds
 
         hidden_states = self.drop(hidden_states)
 
         output_shape = input_shape + (hidden_states.size(-1),)
 
-        presents = () if use_cache else None
+        present_keys = hidden_states.new_zeros(batch_size, self.num_heads, self.num_layers, input_shape[1], self.head_dim) if use_cache else None
+        present_values = hidden_states.new_zeros(batch_size, self.num_heads, self.num_layers, input_shape[1], self.head_dim) if use_cache else None
         all_self_attentions = () if output_attentions else None
         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None
         all_hidden_states = () if output_hidden_states else None
-        for i, (block, layer_past) in enumerate(zip(self.h, past_key_values)):
+        for i, block in enumerate(self.h):
+            if past_keys is not None:
+                key_past = past_keys[:, :, i, :, :].transpose(1, 2)
+                value_past = past_values[:, :, i, :, :].transpose(1, 2)
+                layer_past = tuple([key_past, value_past])
+            else:
+                layer_past = None
 
             # Model parallel
             if self.model_parallel:
@@ -808,7 +826,8 @@ class GPT2Model(GPT2PreTrainedModel):
 
             hidden_states = outputs[0]
             if use_cache is True:
-                presents = presents + (outputs[1],)
+                present_keys[:, :, i, :, :] = outputs[1][0][:, :, past_length:, : ]
+                present_values[:, :, i, :, :] = outputs[1][1][:, :, past_length:, : ]
 
             if output_attentions:
                 all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)
@@ -828,20 +847,24 @@ class GPT2Model(GPT2PreTrainedModel):
         if output_hidden_states:
             all_hidden_states = all_hidden_states + (hidden_states,)
 
-        if not return_dict:
-            return tuple(
-                v
-                for v in [hidden_states, presents, all_hidden_states, all_self_attentions, all_cross_attentions]
-                if v is not None
-            )
+        #if not return_dict:
+        #    return tuple(
+        #        v
+        #        for v in [hidden_states, presents, all_hidden_states, all_self_attentions, all_cross_attentions]
+        #        if v is not None
+        #    )
+
+        #return BaseModelOutputWithPastAndCrossAttentions(
+        #    last_hidden_state=hidden_states,
+        #    past_key_values=presents,
+        #   hidden_states=all_hidden_states,
+        #    attentions=all_self_attentions,
+        #    cross_attentions=all_cross_attentions,
+        #)
+        if use_cache:
+            return hidden_states, present_keys.transpose(1, 3), present_values.transpose(1, 3)
 
-        return BaseModelOutputWithPastAndCrossAttentions(
-            last_hidden_state=hidden_states,
-            past_key_values=presents,
-            hidden_states=all_hidden_states,
-            attentions=all_self_attentions,
-            cross_attentions=all_cross_attentions,
-        )
+        return hidden_states
 
 
 @add_start_docstrings(
